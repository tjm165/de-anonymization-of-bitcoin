{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading for the presentation...\n",
    "\n",
    "1. https://en.bitcoin.it/wiki/Privacy Read about \"Unnecessary input heuristic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Small Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heuristics import *\n",
    "from data_util import *\n",
    "from cluster import *\n",
    "\n",
    "input1 = Input(\"address1\", 2)\n",
    "input2 = Input(\"address2\", 3)\n",
    "output1 = Output(\"address5\", 4)\n",
    "output2 = Output(\"address6\", 1)\n",
    "\n",
    "t1 = Transaction([input1, input2], [output1, output2])\n",
    "\n",
    "my_clusters = cluster([t1], heuristics.optimal_change)\n",
    "print(my_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters(my_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Format raw data\n",
    "Data is obtained from: https://www.kaggle.com/xblock/bitcoin-partial-transaction-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_data('archive/dataset3_2016_1_1500000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run The Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cluster\n",
    "from data_util import load_data\n",
    "import heuristics\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "data = load_data('dataset3_2016_1_1500000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_pickle(heuristic, threshold=None):\n",
    "    clusters = cluster.cluster(data, heuristic, threshold)\n",
    "    \n",
    "    file_name = 'dataset3_' + heuristic.__name__ + '.pkl'\n",
    "\n",
    "    if not clusters:\n",
    "        with open(file_name, 'rb') as f:\n",
    "            clusters = pickle.load(f)\n",
    "    else:\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(clusters, f)\n",
    "            \n",
    "    return clusters\n",
    "    \n",
    "def plot_and_analyze(clusters_list, title):\n",
    "    # Count the number of clusters at a given size\n",
    "    cluster_sizes = defaultdict(int)\n",
    "    total_addr = 0\n",
    "    for cluster in clusters_list:\n",
    "        cluster_size = len(cluster)\n",
    "        total_addr += cluster_size\n",
    "        cluster_sizes[cluster_size] += 1\n",
    "\n",
    "    sizes = list(cluster_sizes.keys())\n",
    "    counts = list(cluster_sizes.values())\n",
    "    avg_size = sum(size * count for size, count in zip(sizes, counts)) / sum(counts)\n",
    "    avg_non_single_size = sum(size * count for size, count in zip(sizes, counts) if size > 1) / \\\n",
    "                          sum(count for size, count in zip(sizes, counts) if size > 1)\n",
    "    print('Total number of addresses:          ', total_addr)\n",
    "    print('Total number of clusters:           ', len(clusters_list))\n",
    "    print('Number of single-address clusters:  ', cluster_sizes[1])\n",
    "    print('Size of largest cluster:            ', max(sizes))\n",
    "    print('Average cluster size:               ', avg_size)\n",
    "    print('Average cluster size (excluding single-address clusters): ', avg_non_single_size)\n",
    "    \n",
    "    # Get cluster size counts in particular range\n",
    "    x, y = zip(*[(size, count) for size, count in cluster_sizes.items() if 100 > size > 1])\n",
    "\n",
    "    # Plot\n",
    "    plt.title(title)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.bar(x, y)\n",
    "    plt.ylabel('Number of Clusters')\n",
    "    plt.xlabel('Number of addresses in cluster')\n",
    "    plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_input_clusters = cluster_and_pickle(heuristics.multi_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_change_clusters = cluster_and_pickle(heuristics.optimal_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_input_opimal_change_clusters = cluster_and_pickle(heuristics.multi_input_optimal_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_clusters = cluster_and_pickle(heuristics.shadow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopefully don't need this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain list of clusters from data\n",
    "start_time = time.time()\n",
    "clusters = cluster.cluster(data, multi_input)\n",
    "print('Elapsed Time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load cluster data for convenience\n",
    "file_name = 'dataset3_muti_input.pkl'\n",
    "\n",
    "if not clusters:\n",
    "    with open(file_name, 'rb') as f:\n",
    "        clusters = pickle.load(f)\n",
    "else:\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(clusters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the number of clusters at a given size\n",
    "cluster_sizes = defaultdict(int)\n",
    "total_addr = 0\n",
    "for cluster in clusters:\n",
    "    cluster_size = len(cluster)\n",
    "    total_addr += cluster_size\n",
    "    cluster_sizes[cluster_size] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sizes = list(cluster_sizes.keys())\n",
    "counts = list(cluster_sizes.values())\n",
    "avg_size = sum(size * count for size, count in zip(sizes, counts)) / sum(counts)\n",
    "avg_non_single_size = sum(size * count for size, count in zip(sizes, counts) if size > 1) / \\\n",
    "                      sum(count for size, count in zip(sizes, counts) if size > 1)\n",
    "print('Total number of addresses:          ', total_addr)\n",
    "print('Total number of clusters:           ', len(clusters))\n",
    "print('Number of single-address clusters:  ', cluster_sizes[1])\n",
    "print('Size of largest cluster:            ', max(sizes))\n",
    "print('Average cluster size:               ', avg_size)\n",
    "print('Average cluster size (excluding single-address clusters): ', avg_non_single_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get cluster size counts in particular range\n",
    "x, y = zip(*[(size, count) for size, count in cluster_sizes.items() if 100 > size > 1])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x, y)\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.xlabel('Number of addresses in cluster')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make a histogram for shadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is ticket number 13 in github\n",
    "\n",
    "# run __get_shadow_data(transactions) on the real data\n",
    "# then make a histogram of the output\n",
    "\n",
    "# based on the histogram, do you think we have a clear distinction of who is a merchant? \n",
    "# Or will we need to make an assumption on our own?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "data = parse_data('archive/dataset3_2016_1_1500000')\n",
    "print('Elapsed Time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "start_time = time.time()\n",
    "data = load_data('dataset3_2016_1_1500000.pkl')\n",
    "print('Elapsed Time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = heuristics.__get_shadow_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = []\n",
    "merchant_counts = []\n",
    "for r in results:\n",
    "    addresses.append(r)\n",
    "    merchant_counts.append(results[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "\n",
    "for c in merchant_counts:\n",
    "    if c in counts:\n",
    "        counts[c] += 1\n",
    "    else:\n",
    "        counts[c] = 1\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(counts.values())[3:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score # maybe use adjusted_rand_score if need\n",
    "# [[1, 2, 3], [4, 5], [6]] -> [0, 0, 0, 1, 1, 2]\n",
    "adjusted_rand_score([0, 0, 2, 2], [1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement split/join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
